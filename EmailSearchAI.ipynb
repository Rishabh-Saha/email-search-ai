{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd389574",
   "metadata": {},
   "source": [
    "# EmailSearchAI — Single‑File RAG over Kaggle Email Threads\n",
    "_Lightweight notebook: ingest ▶︎ chunk ▶︎ embed ▶︎ hybrid search+rerank ▶︎ grounded answer._\n",
    "\n",
    "> Structured similarly to your sample notebook sections: ## Text Processing, ## Chunking & Embeddings, ## Generate and Store Embeddings using OpenAI and ChromaDB, ## Semantic Search with Cache, ## Re-Ranking with a Cross Encoder, ## 6. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023d086",
   "metadata": {},
   "source": [
    "## 1) Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers sentence-transformers torch huggingface_hub\n",
    "# !pip install torch==2.1.0 transformers==4.36.0 huggingface_hub==0.19.4\n",
    "# !pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da8d8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running fresh, uncomment installs (kept inline to stay single-file)\n",
    "# %pip install -q python-dotenv pandas orjson beautifulsoup4 rapidfuzz rank-bm25 chromadb faiss-cpu sentence-transformers openai langchain-text-splitters tiktoken rich\n",
    "\n",
    "import os, json, time, hashlib, sqlite3, math, textwrap\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Optional: load OPENAI_API_KEY from env if present\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c64fe2",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb746087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths (edit if needed) ===\n",
    "RAW_DETAILS = \"dataset/CSV/email_thread_details.csv\"          # from Kaggle dataset\n",
    "RAW_SUMMARIES = \"dataset/CSV/email_thread_summaries.csv\"      # optional for eval\n",
    "CURATED_MSGS = \"./data/curated/emails.jsonl\"\n",
    "CURATED_SUMS = \"./data/curated/thread_summaries.jsonl\"\n",
    "DERIVED_CHUNKS = \"./data/derived/chunks.jsonl\"\n",
    "INDEX_DIR = \"./.chroma_emailsearch\"\n",
    "\n",
    "# === Models ===\n",
    "USE_OPENAI_EMBED = False  # set True to use OpenAI embeddings\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"     # local fast default\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# === Search params ===\n",
    "TOP_K = 12\n",
    "TOP_K_RERANK = 5\n",
    "BM25_TOP = 60\n",
    "\n",
    "# === Cache ===\n",
    "CACHE_PATH = \".cache_emailsearch.sqlite\"\n",
    "CACHE_TTL = 6*3600\n",
    "\n",
    "os.makedirs(\"data/curated\", exist_ok=True)\n",
    "os.makedirs(\"data/derived\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939e0b0",
   "metadata": {},
   "source": [
    "## 3) Ingestion — CSV ▶︎ Curated JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "857fde20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/CSV/email_thread_details.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m         display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sums)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m summaries →** `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURATED_SUMS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Run once after placing CSVs under dataset/raw/\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m ingest_kaggle()\n",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m, in \u001b[0;36mingest_kaggle\u001b[0;34m(details_path, summaries_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mingest_kaggle\u001b[39m(details_path\u001b[38;5;241m=\u001b[39mRAW_DETAILS, summaries_path\u001b[38;5;241m=\u001b[39mRAW_SUMMARIES):\n\u001b[0;32m---> 21\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(details_path)\n\u001b[1;32m     22\u001b[0m     rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/CSV/email_thread_details.csv'"
     ]
    }
   ],
   "source": [
    "def html_to_text(x: str) -> str:\n",
    "    if not isinstance(x, str): return \"\"\n",
    "    return BeautifulSoup(x, \"html.parser\").get_text(\" \", strip=True)\n",
    "\n",
    "def normalize_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if not isinstance(x, str) or not x.strip(): return []\n",
    "    try:\n",
    "        v = json.loads(x)\n",
    "        return v if isinstance(v, list) else [str(v)]\n",
    "    except Exception:\n",
    "        import re\n",
    "        return [s.strip() for s in re.split(r\"[;,]\", x) if s.strip()]\n",
    "\n",
    "def write_jsonl(path, rows):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def ingest_kaggle(details_path=RAW_DETAILS, summaries_path=RAW_SUMMARIES):\n",
    "    df = pd.read_csv(details_path)\n",
    "    rows = []\n",
    "    for i, r in df.iterrows():\n",
    "        thread_id = str(r.get(\"thread_id\",\"\"))\n",
    "        rec = {\n",
    "            \"message_id\": str(r.get(\"message_id\",\"\")) or f\"{thread_id}-{i}\",\n",
    "            \"thread_id\": thread_id,\n",
    "            \"date\": str(r.get(\"timestamp\",\"\")),\n",
    "            \"from\": r.get(\"from\",\"\"),\n",
    "            \"to\": normalize_list(r.get(\"to\",\"\")),\n",
    "            \"cc\": normalize_list(r.get(\"cc\",\"\")) if \"cc\" in df.columns else [],\n",
    "            \"subject\": r.get(\"subject\",\"\"),\n",
    "            \"body_text\": html_to_text(r.get(\"body\",\"\")),\n",
    "            \"body_html\": None,\n",
    "            \"attachments\": [],\n",
    "            \"tags\": [],\n",
    "            \"path\": f\"email_thread_details.csv:{i}\"\n",
    "        }\n",
    "        rows.append(rec)\n",
    "    write_jsonl(CURATED_MSGS, rows)\n",
    "    display(Markdown(f\"**Wrote {len(rows)} messages →** `{CURATED_MSGS}`\"))\n",
    "\n",
    "    if os.path.exists(summaries_path):\n",
    "        ds = pd.read_csv(summaries_path)\n",
    "        sums = [{\"thread_id\": str(sr.get(\"thread_id\",\"\")), \"summary\": sr.get(\"summary\",\"\")} for _, sr in ds.iterrows()]\n",
    "        write_jsonl(CURATED_SUMS, sums)\n",
    "        display(Markdown(f\"**Wrote {len(sums)} summaries →** `{CURATED_SUMS}`\"))\n",
    "\n",
    "# Run once after placing CSVs under dataset/raw/\n",
    "ingest_kaggle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c662f",
   "metadata": {},
   "source": [
    "## 4) Chunking — Message, Thread‑Window, or Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def iter_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def windowed_chunks(text, max_toks=900, overlap=120):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=max_toks, chunk_overlap=overlap, separators=[\"\\n\\n\",\"\\n\",\". \"])\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "def chunk_messages(curated_path=CURATED_MSGS, out_path=DERIVED_CHUNKS, mode=\"thread_window\"):\n",
    "    # mode: \"message\", \"thread_window\"\n",
    "    msgs = list(iter_jsonl(curated_path))\n",
    "    msgs.sort(key=lambda r: (r[\"thread_id\"], r.get(\"date\",\"\")))\n",
    "\n",
    "    chunks = []\n",
    "    if mode == \"message\":\n",
    "        for m in msgs:\n",
    "            text = f\"SUBJECT: {m['subject']}\\nFROM: {m['from']}\\nDATE: {m['date']}\\n\\n{m['body_text']}\"\n",
    "            for piece in windowed_chunks(text, 900, 120):\n",
    "                chunks.append({\n",
    "                    \"id\": f\"{m['message_id']}\",\n",
    "                    \"thread_id\": m[\"thread_id\"],\n",
    "                    \"subject\": m[\"subject\"],\n",
    "                    \"date\": m[\"date\"],\n",
    "                    \"text\": piece\n",
    "                })\n",
    "    else:  # thread_window\n",
    "        # merge consecutive messages within a thread up to target token-ish length\n",
    "        buf = []\n",
    "        cur_tid = None\n",
    "        def flush():\n",
    "            nonlocal buf, chunks, cur_tid\n",
    "            if not buf: return\n",
    "            text = \"\\n\\n---\\n\\n\".join(buf)\n",
    "            pieces = windowed_chunks(text, 1000, 140)\n",
    "            for j, p in enumerate(pieces):\n",
    "                chunks.append({\n",
    "                    \"id\": f\"{cur_tid}-{hashlib.md5((p[:200]).encode()).hexdigest()[:8]}-{j}\",\n",
    "                    \"thread_id\": cur_tid,\n",
    "                    \"subject\": subj_agg[:180],\n",
    "                    \"date\": f\"{d0}…{d1}\",\n",
    "                    \"text\": p\n",
    "                })\n",
    "            buf = []\n",
    "\n",
    "        subj_agg, d0, d1 = \"\", \"\", \"\"\n",
    "        for m in msgs:\n",
    "            tid = m[\"thread_id\"]\n",
    "            if cur_tid is None:\n",
    "                cur_tid = tid\n",
    "                subj_agg = m[\"subject\"]\n",
    "                d0 = d1 = m.get(\"date\",\"\")\n",
    "            if tid != cur_tid:\n",
    "                flush()\n",
    "                cur_tid = tid\n",
    "                subj_agg = m[\"subject\"]\n",
    "                d0 = d1 = m.get(\"date\",\"\")\n",
    "            d1 = m.get(\"date\",\"\") or d1\n",
    "            buf.append(f\"SUBJECT: {m['subject']}\\nFROM: {m['from']}\\nDATE: {m['date']}\\n\\n{m['body_text']}\")\n",
    "        flush()\n",
    "\n",
    "    write_jsonl(out_path, chunks)\n",
    "    display(Markdown(f\"**Chunks written: {len(chunks)} →** `{out_path}`\"))\n",
    "\n",
    "chunk_messages(mode=\"thread_window\")  # run after ingest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20167ca8",
   "metadata": {},
   "source": [
    "## 5) Embeddings & Index (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2051ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fa3cdaf1fa4e3bb86bad1949815282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c7f97d18764f4298fe66d82885fbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5891d85e774410b70a91b981ec26ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b495cf6c648718341dde3a882f236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea654e5c7fc143d3bdd07155b8875335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec37942dbda4088a2180bde35b4a146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3493e78bc26d46d7a6db321f357a5665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fb3b9dce37480596bcc6bbb7eb2e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc31b353c43044c4b658edbdb3b15425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c1d5f7311d49b4a25427a22df67dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5144869917b4a2d91c608af15e6e022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda6d75dbaf4d2eb051038f08419181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a271358a7641238a6f1a18285ff4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f5e8b8b9c44876a29dfb79aac4f576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f7a778d9cf43bc8639098f15845cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b20f20115240d5819052026be6abdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bec186675b47919ced92d2b6983784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd579275fd046b386c18b2083e33950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.xml: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281185838a104d30819cd9823613e2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d1db2496ff42c1872ce5f6655b730b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.xml: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8455f522e67e45b18f62324a60b1ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e66c414eda54973a263e2f4df2a1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d767e494fe65456eb4cbae2405220df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b823dd81f94b47a55ec2d169c28a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac26029874974da78d5dd2b022997116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831c1f1bcefa4abfa348c9cafea5eeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6fc099b59049a49e42425e034b2fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda5291678d246f1b1cab0c95a12ebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/derived/chunks.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m         col\u001b[38;5;241m.\u001b[39madd(ids\u001b[38;5;241m=\u001b[39mids, documents\u001b[38;5;241m=\u001b[39mtexts, embeddings\u001b[38;5;241m=\u001b[39mvecs, metadatas\u001b[38;5;241m=\u001b[39mmetas)\n\u001b[1;32m     41\u001b[0m     display(Markdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Index built in** `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(index_dir)))\n\u001b[0;32m---> 43\u001b[0m build_index()\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mbuild_index\u001b[0;34m(chunks_path, index_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m col \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     29\u001b[0m ids, texts, metas \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iter_jsonl(chunks_path)):\n\u001b[1;32m     31\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36miter_jsonl\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_jsonl\u001b[39m(path):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/derived/chunks.jsonl'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name=EMBED_MODEL, use_openai=USE_OPENAI_EMBED):\n",
    "        self.kind = \"openai\" if use_openai else \"st\"\n",
    "        if self.kind == \"st\":\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "        else:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "            self.model_name = model_name.split(\"/\",1)[1]\n",
    "\n",
    "    def encode(self, texts: List[str]):\n",
    "        if self.kind == \"st\":\n",
    "            return self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False).tolist()\n",
    "        resp = self.client.embeddings.create(model=self.model_name, input=texts)\n",
    "        return [e.embedding for e in resp.data]\n",
    "\n",
    "def build_index(chunks_path=DERIVED_CHUNKS, index_dir=INDEX_DIR):\n",
    "    emb = Embedder()\n",
    "    client = PersistentClient(path=index_dir)\n",
    "    try:\n",
    "        client.delete_collection(\"emails\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    col = client.create_collection(\"emails\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    ids, texts, metas = [], [], []\n",
    "    for i, r in enumerate(iter_jsonl(chunks_path)):\n",
    "        ids.append(r[\"id\"] if r.get(\"id\") else f\"c{i}\")\n",
    "        texts.append(r[\"text\"])\n",
    "        metas.append({k:v for k,v in r.items() if k != \"text\"})\n",
    "        if len(ids) >= 256:\n",
    "            vecs = emb.encode(texts)\n",
    "            col.add(ids=ids, documents=texts, embeddings=vecs, metadatas=metas)\n",
    "            ids, texts, metas = [], [], []\n",
    "    if ids:\n",
    "        vecs = emb.encode(texts)\n",
    "        col.add(ids=ids, documents=texts, embeddings=vecs, metadatas=metas)\n",
    "    display(Markdown(\"**Index built in** `{}'`\".format(index_dir)))\n",
    "\n",
    "build_index()  # run after chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d767720",
   "metadata": {},
   "source": [
    "## 6) Search — Hybrid (Vector ∪ BM25) + TTL Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def ensure_cache(path=CACHE_PATH):\n",
    "    conn = sqlite3.connect(path)\n",
    "    conn.execute(\"CREATE TABLE IF NOT EXISTS cache (k TEXT PRIMARY KEY, v BLOB, t INT)\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def cache_get(k, path=CACHE_PATH):\n",
    "    conn = sqlite3.connect(path)\n",
    "    cur = conn.execute(\"SELECT v,t FROM cache WHERE k=?\", (k,))\n",
    "    row = cur.fetchone()\n",
    "    conn.close()\n",
    "    if not row: return None\n",
    "    v, t = row\n",
    "    if time.time() - t > CACHE_TTL: return None\n",
    "    return json.loads(v)\n",
    "\n",
    "def cache_put(k, obj, path=CACHE_PATH):\n",
    "    conn = sqlite3.connect(path)\n",
    "    conn.execute(\"REPLACE INTO cache(k,v,t) VALUES (?,?,?)\", (k, json.dumps(obj), int(time.time())))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def qhash(q: str) -> str:\n",
    "    return hashlib.sha256(q.strip().lower().encode()).hexdigest()\n",
    "\n",
    "# Preload BM25 corpus from chunks\n",
    "_bm25 = None\n",
    "_bm25_docs = None\n",
    "def _load_bm25(chunks_path=DERIVED_CHUNKS):\n",
    "    global _bm25, _bm25_docs\n",
    "    docs = [r[\"text\"] for r in iter_jsonl(chunks_path)]\n",
    "    tokenized = [d.split() for d in docs]\n",
    "    _bm25 = BM25Okapi(tokenized)\n",
    "    _bm25_docs = docs\n",
    "\n",
    "def vector_search(query: str, top_k=TOP_K):\n",
    "    client = PersistentClient(path=INDEX_DIR)\n",
    "    col = client.get_collection(\"emails\")\n",
    "    res = col.query(query_texts=[query], n_results=top_k*2, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    out = []\n",
    "    for i in range(len(res[\"ids\"][0])):\n",
    "        out.append({ \"text\": res[\"documents\"][0][i], **res[\"metadatas\"][0][i] })\n",
    "    return out[:top_k]\n",
    "\n",
    "def bm25_search(query: str, top_bm25=BM25_TOP):\n",
    "    if _bm25 is None: _load_bm25()\n",
    "    scores = _bm25.get_scores(query.split())\n",
    "    idxs = sorted(range(len(scores)), key=lambda i: -scores[i])[:top_bm25]\n",
    "    outs = []\n",
    "    # Reopen chunks to fetch metadata\n",
    "    all_chunks = list(iter_jsonl(DERIVED_CHUNKS))\n",
    "    for i in idxs:\n",
    "        r = all_chunks[i]\n",
    "        outs.append({ \"text\": r[\"text\"], **{k:v for k,v in r.items() if k != \"text\"} })\n",
    "    return outs\n",
    "\n",
    "def hybrid_search(query: str, top_k=TOP_K):\n",
    "    key = qhash(\"hybrid:\"+query)\n",
    "    ensure_cache()\n",
    "    hit = cache_get(key)\n",
    "    if hit: return hit\n",
    "    v = vector_search(query, top_k=top_k)\n",
    "    b = bm25_search(query, top_bm25=BM25_TOP)\n",
    "    # Union by (id, text) signature\n",
    "    seen = set()\n",
    "    merged = []\n",
    "    for lst in (v, b):\n",
    "        for c in lst:\n",
    "            sig = (c.get(\"id\"), c[\"text\"][:64])\n",
    "            if sig in seen: continue\n",
    "            seen.add(sig); merged.append(c)\n",
    "    cache_put(key, merged)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd147a89",
   "metadata": {},
   "source": [
    "## 7) Re‑ranking (Cross‑Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ba577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "_reranker = None\n",
    "def get_reranker(model=CROSS_ENCODER_MODEL):\n",
    "    global _reranker\n",
    "    if _reranker is None:\n",
    "        _reranker = CrossEncoder(model)\n",
    "    return _reranker\n",
    "\n",
    "def rerank(query: str, candidates: List[Dict[str,Any]], top_k=TOP_K_RERANK):\n",
    "    ce = get_reranker()\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = ce.predict(pairs)\n",
    "    for s, c in zip(scores, candidates):\n",
    "        c[\"rerank_score\"] = float(s)\n",
    "    ranked = sorted(candidates, key=lambda x: -x[\"rerank_score\"])\n",
    "    return ranked[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36997e",
   "metadata": {},
   "source": [
    "## 8) Generation — Grounded Answer (Quotes + Provenance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def make_prompt(query: str, chunks: List[Dict[str,Any]]) -> str:\n",
    "        ctx = \"\\n\\n\".join([\n",
    "            f\"[CHUNK {i+1}]\\nTHREAD: {c.get('thread_id','?')} | DATE: {c.get('date','?')}\\nSUBJECT: {c.get('subject','')}\\nTEXT:\\n{c['text']}\"\n",
    "            for i, c in enumerate(chunks)\n",
    "        ])\n",
    "        return f\"\"\"\n",
    "Answer the user strictly from the CONTEXT.\n",
    "Rules:\n",
    "- Short answer first.\n",
    "- Then Evidence: bullet list; each bullet quotes a short line (≤20 words) and includes (thread_id · date).\n",
    "- If context is insufficient or conflicting, say so explicitly.\n",
    "- Do not add external knowledge.\n",
    "\n",
    "USER QUESTION: {query}\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\"\"\"\n",
    "\n",
    "    def generate_answer_openai(query, chunks):\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        prompt = make_prompt(query, chunks)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"You are EmailSearchAI, a cautious, citation-first assistant.\"},\n",
    "                {\"role\":\"user\",\"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697aeab",
   "metadata": {},
   "source": [
    "## 9) Run a Query End‑to‑End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query: str, top_k=TOP_K, top_k_rerank=TOP_K_RERANK, show_top=3):\n",
    "    cand = hybrid_search(query, top_k=top_k)\n",
    "    ranked = rerank(query, cand, top_k=top_k_rerank)\n",
    "    display(Markdown(\"**Top candidates (after re‑rank):**\"))\n",
    "    for i, c in enumerate(ranked[:show_top]):\n",
    "        md = f\"**{i+1}. THREAD {c.get('thread_id','?')} — {c.get('date','')}**  \\n\" \\\n",
    "             f\"*{c.get('subject','')[:200]}*\\n\\n> {c['text'][:500]}...\"\n",
    "        display(Markdown(md))\n",
    "    if not OPENAI_API_KEY:\n",
    "        display(Markdown(\"⚠️ Set OPENAI_API_KEY to enable final LLM answer.\"))\n",
    "        return ranked\n",
    "    ans = generate_answer_openai(query, ranked[:show_top])\n",
    "    display(Markdown(\"### Final Answer\"))\n",
    "    display(Markdown(ans))\n",
    "    return ranked\n",
    "\n",
    "# Example:\n",
    "# answer(\"What decision was reached about the Q3 forecast and who approved it?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3912853",
   "metadata": {},
   "source": [
    "## 10) (Optional) Quick Multi‑Query Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Query: **Summarize the main decision and the final deadline in the budget thread.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotFoundError",
     "evalue": "Collection [emails] does not exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# SAMPLE QUERIES (edit to match your corpus topics)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m SAMPLE_QUERIES \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the main decision and the final deadline in the budget thread.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho proposed the chosen approach for the data migration and when?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList follow-up action items assigned to Finance in November threads.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n\u001b[0;32m---> 14\u001b[0m run_queries(SAMPLE_QUERIES)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mrun_queries\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m      4\u001b[0m     display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Query: **\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m     results[q] \u001b[38;5;241m=\u001b[39m answer(q)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36manswer\u001b[0;34m(query, top_k, top_k_rerank, show_top)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k\u001b[38;5;241m=\u001b[39mTOP_K, top_k_rerank\u001b[38;5;241m=\u001b[39mTOP_K_RERANK, show_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     cand \u001b[38;5;241m=\u001b[39m hybrid_search(query, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m      3\u001b[0m     ranked \u001b[38;5;241m=\u001b[39m rerank(query, cand, top_k\u001b[38;5;241m=\u001b[39mtop_k_rerank)\n\u001b[1;32m      4\u001b[0m     display(Markdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Top candidates (after re‑rank):**\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[7], line 64\u001b[0m, in \u001b[0;36mhybrid_search\u001b[0;34m(query, top_k)\u001b[0m\n\u001b[1;32m     62\u001b[0m hit \u001b[38;5;241m=\u001b[39m cache_get(key)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hit: \u001b[38;5;28;01mreturn\u001b[39;00m hit\n\u001b[0;32m---> 64\u001b[0m v \u001b[38;5;241m=\u001b[39m vector_search(query, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m     65\u001b[0m b \u001b[38;5;241m=\u001b[39m bm25_search(query, top_bm25\u001b[38;5;241m=\u001b[39mBM25_TOP)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Union by (id, text) signature\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mvector_search\u001b[0;34m(query, top_k)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvector_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k\u001b[38;5;241m=\u001b[39mTOP_K):\n\u001b[1;32m     39\u001b[0m     client \u001b[38;5;241m=\u001b[39m PersistentClient(path\u001b[38;5;241m=\u001b[39mINDEX_DIR)\n\u001b[0;32m---> 40\u001b[0m     col \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     res \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mquery(query_texts\u001b[38;5;241m=\u001b[39m[query], n_results\u001b[38;5;241m=\u001b[39mtop_k\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/client.py:201\u001b[0m, in \u001b[0;36mClient.get_collection\u001b[0;34m(self, name, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_collection\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 201\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mget_collection(\n\u001b[1;32m    202\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    203\u001b[0m         tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    204\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m     persisted_ef_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfiguration_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m     validate_embedding_function_conflict_on_get(\n\u001b[1;32m    209\u001b[0m         embedding_function, persisted_ef_config\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/rust.py:250\u001b[0m, in \u001b[0;36mRustBindingsAPI.get_collection\u001b[0;34m(self, name, tenant, database)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_collection\u001b[39m(\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CollectionModel:\n\u001b[0;32m--> 250\u001b[0m     collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings\u001b[38;5;241m.\u001b[39mget_collection(name, tenant, database)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    253\u001b[0m         name\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         database\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    259\u001b[0m     )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Collection [emails] does not exists"
     ]
    }
   ],
   "source": [
    "def run_queries(queries: List[str]):\n",
    "    results = {}\n",
    "    for q in queries:\n",
    "        display(Markdown(f\"## Query: **{q}**\"))\n",
    "        results[q] = answer(q)\n",
    "    return results\n",
    "\n",
    "# SAMPLE QUERIES (edit to match your corpus topics)\n",
    "SAMPLE_QUERIES = [\n",
    "    \"Summarize the main decision and the final deadline in the budget thread.\",\n",
    "    \"Who proposed the chosen approach for the data migration and when?\",\n",
    "    \"List follow-up action items assigned to Finance in November threads.\",\n",
    "]\n",
    "run_queries(SAMPLE_QUERIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda39f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
